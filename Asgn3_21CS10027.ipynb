{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://drive.google.com/uc?id=1lALXyvaUzsViclLbIvEJr_AtFEdZdstp\")\n",
    "x = data.drop('MEDV', axis=1)\n",
    "x=x.to_numpy()\n",
    "y = data['MEDV']\n",
    "y=y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.575,  4.98 , 15.3  ],\n",
       "       [ 6.421,  9.14 , 17.8  ],\n",
       "       [ 7.185,  4.03 , 17.8  ],\n",
       "       ...,\n",
       "       [ 6.976,  5.64 , 21.   ],\n",
       "       [ 6.794,  6.48 , 21.   ],\n",
       "       [ 6.03 ,  7.88 , 21.   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 504000.,  453600.,  728700.,  701400.,  760200.,  602700.,\n",
       "        480900.,  569100.,  346500.,  396900.,  315000.,  396900.,\n",
       "        455700.,  428400.,  382200.,  417900.,  485100.,  367500.,\n",
       "        424200.,  382200.,  285600.,  411600.,  319200.,  304500.,\n",
       "        327600.,  291900.,  348600.,  310800.,  386400.,  441000.,\n",
       "        266700.,  304500.,  277200.,  275100.,  283500.,  396900.,\n",
       "        420000.,  441000.,  518700.,  646800.,  732900.,  558600.,\n",
       "        531300.,  518700.,  445200.,  405300.,  420000.,  348600.,\n",
       "        302400.,  407400.,  413700.,  430500.,  525000.,  491400.,\n",
       "        396900.,  743400.,  518700.,  663600.,  489300.,  411600.,\n",
       "        392700.,  336000.,  466200.,  525000.,  693000.,  493500.,\n",
       "        407400.,  462000.,  365400.,  438900.,  508200.,  455700.,\n",
       "        478800.,  491400.,  506100.,  449400.,  420000.,  436800.,\n",
       "        445200.,  426300.,  588000.,  501900.,  520800.,  480900.,\n",
       "        501900.,  558600.,  472500.,  466200.,  495600.,  602700.,\n",
       "        474600.,  462000.,  480900.,  525000.,  432600.,  596400.,\n",
       "        449400.,  812700.,  919800.,  697200.,  577500.,  556500.,\n",
       "        390600.,  405300.,  422100.,  409500.,  409500.,  428400.,\n",
       "        415800.,  407400.,  455700.,  478800.,  394800.,  392700.,\n",
       "        388500.,  384300.,  445200.,  403200.,  428400.,  405300.,\n",
       "        462000.,  426300.,  430500.,  363300.,  394800.,  449400.,\n",
       "        329700.,  340200.,  378000.,  300300.,  403200.,  411600.,\n",
       "        483000.,  386400.,  327600.,  380100.,  365400.,  359100.,\n",
       "        279300.,  373800.,  294000.,  302400.,  281400.,  327600.,\n",
       "        247800.,  289800.,  327600.,  306600.,  373800.,  323400.,\n",
       "        451500.,  411600.,  321300.,  407400.,  357000.,  327600.,\n",
       "        275100.,  867300.,  510300.,  489300.,  567000.,  476700.,\n",
       "        525000.,  499800.,  499800.,  468300.,  365400.,  401100.,\n",
       "        485100.,  495600.,  474600.,  617400.,  487200.,  516600.,\n",
       "        627900.,  781200.,  835800.,  760200.,  795900.,  682500.,\n",
       "        554400.,  621600.,  672000.,  625800.,  732900.,  777000.,\n",
       "        640500.,  764400.,  653100.,  611100.,  699300.,  636300.,\n",
       "        726600.,  732900.,  690900.,  506100.,  888300., 1018500.,\n",
       "        474600.,  512400.,  472500.,  512400.,  420000.,  455700.,\n",
       "        405300.,  470400.,  590100.,  497700.,  525000.,  489300.,\n",
       "        602700.,  451500.,  483000.,  560700.,  455700.,  577500.,\n",
       "        632100.,  940800.,  789600.,  663600.,  980700.,  661500.,\n",
       "        510300.,  665700.,  875700., 1014300.,  609000.,  504000.,\n",
       "        527100.,  661500.,  497700.,  489300.,  462000.,  422100.,\n",
       "        466200.,  497700.,  369600.,  388500.,  510300.,  430500.,\n",
       "        514500.,  550200.,  512400.,  520800.,  621600.,  898800.,\n",
       "        459900.,  438900.,  924000.,  756000.,  632100.,  709800.,\n",
       "        905100., 1024800.,  651000.,  766500.,  478800.,  644700.,\n",
       "        913500.,  434700.,  443100.,  529200.,  512400.,  739200.,\n",
       "        680400.,  672000.,  697200.,  695100.,  611100.,  737100.,\n",
       "        953400.,  743400.,  966000.,  676200.,  462000.,  422100.,\n",
       "        487200.,  468300.,  520800.,  598500.,  783300.,  585900.,\n",
       "        501900.,  455700.,  600600.,  569100.,  426300.,  472500.,\n",
       "        609000.,  520800.,  462000.,  554400.,  695100.,  758100.,\n",
       "        596400.,  701400.,  592200.,  478800.,  426300.,  338100.,\n",
       "        464100.,  407400.,  453600.,  499800.,  340200.,  373800.,\n",
       "        415800.,  485100.,  441000.,  499800.,  485100.,  428400.,\n",
       "        388500.,  525000.,  516600.,  483000.,  466200.,  405300.,\n",
       "        474600.,  415800.,  359100.,  407400.,  466200.,  434700.,\n",
       "        443100.,  409500.,  388500.,  432600.,  399000.,  392700.,\n",
       "        686700.,  346500.,  501900.,  655200.,  367500.,  361200.,\n",
       "        485100.,  514500.,  558600.,  480900.,  506100.,  390600.,\n",
       "        632100.,  382200.,  432600.,  373800.,  455700.,  476700.,\n",
       "        474600.,  525000.,  417900.,  436800.,  352800.,  577500.,\n",
       "        459900.,  485100.,  289800.,  289800.,  315000.,  291900.,\n",
       "        279300.,  275100.,  214200.,  218400.,  228900.,  237300.,\n",
       "        258300.,  184800.,  151200.,  220500.,  155400.,  214200.,\n",
       "        241500.,  317100.,  487200.,  203700.,  289800.,  266700.,\n",
       "        275100.,  262500.,  178500.,  105000.,  132300.,  117600.,\n",
       "        151200.,  254100.,  174300.,  178500.,  105000.,  249900.,\n",
       "        585900.,  361200.,  577500.,  315000.,  361200.,  375900.,\n",
       "        342300.,  147000.,  151200.,  157500.,  218400.,  184800.,\n",
       "        176400.,  350700.,  298200.,  436800.,  281400.,  245700.,\n",
       "        174300.,  214200.,  228900.,  231000.,  199500.,  304500.,\n",
       "        296100.,  338100.,  300300.,  245700.,  281400.,  201600.,\n",
       "        182700.,  176400.,  268800.,  220500.,  359100.,  386400.,\n",
       "        323400.,  226800.,  247800.,  312900.,  264600.,  296100.,\n",
       "        273000.,  281400.,  319200.,  338100.,  373800.,  312900.,\n",
       "        296100.,  266700.,  283500.,  312900.,  420000.,  344400.,\n",
       "        371700.,  409500.,  424200.,  449400.,  417900.,  399000.,\n",
       "        401100.,  401100.,  422100.,  417900.,  411600.,  487200.,\n",
       "        625800.,  289800.,  279300.,  350700.,  252000.,  306600.,\n",
       "        449400.,  483000.,  497700.,  525000.,  457800.,  432600.,\n",
       "        445200.,  401100.,  432600.,  319200.,  147000.,  170100.,\n",
       "        285600.,  422100.,  457800.,  514500.,  485100.,  413700.,\n",
       "        384300.,  445200.,  367500.,  352800.,  470400.,  432600.,\n",
       "        501900.,  462000.,  249900.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    layer1 = sigmoid(np.dot(X, W1) + b1)\n",
    "    output = np.dot(layer1, W2) + b2\n",
    "    return layer1, output\n",
    "\n",
    "\n",
    "def backward(X, y, layer1, output, W1, W2, b1,b2,learning_rate):\n",
    "    loss = output - y\n",
    "    dW2 = np.dot(layer1.T, loss)\n",
    "    db2 = np.sum(loss, axis=0)\n",
    "    d_layer1 = np.dot(loss, W2.T) * sigmoid_derivative(layer1)\n",
    "    dW1 = np.dot(X.T, d_layer1)\n",
    "    db1 = np.sum(d_layer1, axis=0)\n",
    "\n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "def train(X_train, y_train, hidden_size, learning_rate, epochs):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = 1\n",
    "    W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        layer1, output = forward(X_train, W1, b1, W2, b2)\n",
    "        W1, b1, W2, b2 = backward(X_train, y_train, layer1, output, W1, W2, b1,b2,learning_rate)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def evalute(X_test, W1, b1, W2, b2):\n",
    "    _, predictions = forward(X_test, W1, b1, W2, b2)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=24)\n",
    "results = {}\n",
    "\n",
    "\n",
    "hidden_layer_sizes = [3, 4, 5]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Size: 3, Learning Rate: 0.01\n",
      "Mean Squared Error: nan\n",
      "\n",
      "Hidden Layer Size: 4, Learning Rate: 0.001\n",
      "Mean Squared Error: 24065410566.7926\n",
      "\n",
      "Hidden Layer Size: 5, Learning Rate: 0.0001\n",
      "Mean Squared Error: 24755038542.9318\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16394/1067911059.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/tmp/ipykernel_16394/1067911059.py:26: RuntimeWarning: invalid value encountered in multiply\n",
      "  d_layer1 = np.dot(loss, W2.T) * sigmoid_derivative(layer1)\n",
      "/tmp/ipykernel_16394/1067911059.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "/tmp/ipykernel_16394/1067911059.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(hidden_layer_sizes)):\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            x_train, x_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            sca=StandardScaler()\n",
    "            x_train=sca.fit_transform(x_train)\n",
    "            x_test=sca.fit_transform(x_test)\n",
    "            y_test=y_test.reshape(-1,1)\n",
    "            y_train=y_train.reshape(-1,1)\n",
    "\n",
    "        \n",
    "        W1, b1, W2, b2 = train(x_train, y_train, hidden_layer_sizes[i], learning_rates[i], epochs)\n",
    "        y_pre = evalute(x_test, W1, b1, W2, b2)\n",
    "        mean_sq_err = np.mean((y_pre - y_test) ** 2)\n",
    "        print(f\"Hidden Layer Size: {hidden_layer_sizes[i]}, Learning Rate: {learning_rates[i]}\")\n",
    "        print(f\"Mean Squared Error: {mean_sq_err:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
